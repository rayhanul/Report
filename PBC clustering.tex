     
\documentclass[12pt]{report}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb,amsmath}
\usepackage{tabularx,ragged2e,booktabs,caption}
\usepackage{setspace}
\usepackage[a4paper,margin=1in,footskip=.5cm]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{makeidx}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{lscape}


\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\graphicspath{ {images/} }

\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}




\begin{document}
\doublespacing
%A new Package Based Clustering for Enhancing Software Defect Prediction Accuracy
\title{ Software Defect Prediction}
\date{}
\maketitle







%\IEEEcompsoctitleabstractindextext{%
%
%\begin{abstract}
%
%Software defect prediction models considering clustering are to combine related features to enhance the probability of predicting defects. Aggregating related and similar classes is the main challenge in software clustering. An efficient clustering approach named as Package Based Clustering has been proposed to group the software for predicting defects. It uses Object Oriented classes' relationships and similarities to group the software into multiple clusters. To segregate a software project into multiple clusters, it performs textual analysis to identify all Object Oriented classes from the software project. Then it uses package information of each class to divide those into clusters. To analyze the proposed clustering algorithm, the linear regression model is used which learns from clusters of related and similar classes. The experiment has been conducted on JEdit 3.2 and shows that the prediction model using Package Based Clustering is  $54\%$, $71\%$, $90\%$ better than the prediction models built on BorderFlow, k-means and the entire system respectively. 
%
%
 %\end{abstract}
%
%\begin{IEEEkeywords}
%\textbf{keywords} - Software engineering, Software testing, Software defect prediction, Package Based Clustering.
%\end{IEEEkeywords}}
%
%
%\IEEEpeerreviewmaketitle
\chapter{Introduction}
\index{Introduction}
Software defect prediction is a ..........................
\chapter{Background Study and Discussion}
\section{Introduction}

Software Defect Prediction (SDP) is a process of predicting the defects by using the code metrics and bug history of the targeted software to ensure the quality. The Software Quality Assurance (SQA) is a set of activities that ensure a software system to meet a specific quality level. Software companies always concern to maintain the quality of a software and tries hard to find out all the holes before its release. To meet these goals, many researches have already been conducted for building prediction models to predict the software defects. These prediction models are popularly known as SDP models.

%SDP is the process of predicting the defects of a software by using the code metrics and bug history of the  targeted system. SDP helps practitioners in the software development process by identifying the quality of a software. 
%
There are many SDP models for predicting the software defects such as statistical model or machine learning algorithms. Both models are used so frequently by the researchers to predict the defect of a end product. The statistical model are Linear regression, Logistic regression, Step Wise Regression etc. on the other hand, the machine learning algorithms are Support vector machines, Bayesian network, Naive Bayes, Hidden Markov models etc.
After selecting the prediction model from the statistical or machine learning models, the software project data is collected from the software repositories. Then, the selected model is applied on the Dataset to predict the defects that may exists in a software's files. Finally the accuracy and performance of those prediction models are evaluated by using precision, recall, F-measure, coefficient of determination etc. The general overview of SDP model is illustrated in Figure \ref{generalized_Defect_Prediction_Model}. 
%Most of the SDPs try to correctly classify software's component as being defective or non-defective. Other SDPs aim to predict the number of defects that may appear in a software's files or subsystems.
\begin{figure}[h!]
\center
      \includegraphics[angle=0,width=90mm, height=50mm,scale=1]{images/pdf/general_defect_prediction_model.eps}
		\caption{Software defect prediction model}
		\label{generalized_Defect_Prediction_Model}	
\end{figure}
The rest of the Chapter is organized as follows: Section \ref{defination_defect} and \ref{defination_defect_Predictors} presents the defination of software defect and software defect predictors respectively. Section \ref{definition_Defect_Prediction_Model} discusses about the software defect prediction model used in this resarch. Section \ref{software_code_metrics_description} provides the detail description about the selected software code metrics.  
\section{Defination of Software Defect and SDP }
\label{defination_defect}
The software defect is any flaws or errors in a software that produce unexpected results. A software programmer, designer or others, associated with the development of a software can make mistakes or errors while designing or building the software. These mistakes or errors are called defect.
%mean that there are flaws in the software. These

The term defect may vary based on the perception of the software developers, QA teams or Project Managers (PM). For example, bug databases of open source software contain users' request for bug corrections. These requests are frequently considered as feature requests by the developers of those systems. That means what one stakeholder considers as a defect may not be perceived as the same by other stakeholders. In essence, software defect is any kind of deviation from stakeholder's expectation.

\subsection{Software Defect Prediction}
\label{defination_defect_Predictors}
A Software Defect Prediction (SDP) is a method that help the software practitioners by predicting possible defects that the end product might contain. The software defect prediction model, also known as defect predictor uses the software code metrics and the past information to predict defects for the current project. According to Brooks \cite{brooks1995mythical}, half the cost of software development is in unit and systems testing. Harold and Tahat also conform that testing phase requires approximately $50\%$ of the whole project schedule \cite{harrold2000testing,tahat2001requirement}. The defect predictor model is applied to the software software project before starting testing to identify where the defects might exist. This allows PMs to efficiently allocate their scarce resources. 

The SDP model, though works well in predicting the software defect, there exists some critical steps and decisions that are needed to remember when building the defect prediction model. The various ways of defect prediction and challenges that should be in account when predicting defects are described below:

\subsection{Selected Defect Prediction Model}
\label{Ways_Of_the_Software_Defect_Prediction}
The SDP uses a variety of modeling techniques to predict the defects of a software. These prediction model can be categorized to the statistical model and machine learning algorithms. The statistical models formalize the relationship between the independent variables and the dependent variable to predict the dependent variable. On the other hand, machine learning algorithms learns from the experiences that it faces. Both models work well in predicting the defects. Some widely used models are described below:
% A model is usually specified by mathematical equations that relate one or more independent variables and other dependent variables.
%\section{Regression Analysis as a Defect Prediction Model}
%\label{definition_Defect_Prediction_Model}
%The regression analysis is a statistical process for estimating the relationships among the variables. It helps to understand how the values of the dependent variables change when the values of independent variables are changed. There are many techniques for carrying out the regression analysis such as linear regression and ordinary least squares regression, logistic regression, step wise regression, Nonparametric regression etc. 
%
%The performance of the regression analysis depends on the data used to train the regression model and how it relates to the regression model being used. Since the true form of the data is generally unknown, regression analysis often depends to some extent on making assumptions about this data.These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results.

%when any one of the independent variables is varied, while the other independent variables are held fixed.

%Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.
\subsubsection{Linear Regression Analysis}
Linear regression analysis is the process of analyzing the relationships between variables, usually the variables are divided into dependent and independent variables. Let Y denote the dependent variable which value is predicted and $X_{1}$,...,$X_{k}$ denote the independent variables. The linear regression analysis predicts the value of dependent variable (Y) by analyzing the independent variables ($X_{1}$,...,$X_{k}$). Then the equation for computing the predicted value of $Y$ is:
\begin{equation}
\label{eq:linear_regression_analysis}
 Y=b_{1}X_{1}+b_{2}X_{2}+b_{3}X_{3}+...+b_{n}X_{n}+c
\end{equation}
This formula is a straight-line function of each of the X variables, holding the others fixed, and the contributions of different X variables to the predictions are additive. The slopes of their individual straight-line relationships with Y are the constants $b_{1}$, $b_{2}$,..., $b_{k}$, the so-called coefficients of the variables. The coefficient ($b_{i}$) is the change in the predicted value of Y per unit of change in $X_{i}$. The additional constant c, the so-called intercept, is the prediction that the model would make if all the X’s are zero. 

%The coefficients and intercept are estimated by least squares, i.e., setting them equal to the unique values that minimize the sum of squared errors within the sample of data to which the model is fitted.
%
\subsection{Other Defect Prediction Model}

%\subsubsection{Naive Bayes Classifier}
%Naive Bayes is the simple probabilistic classifier, based on Bayes theorem. The classifier calculates a future probability for a class using the prior probability of that class. Given a set of variables $x_{1}, x_{2}, x_{3}..., x_{n}$ and $c_{1},c_{2},c_{3}...,c_{n}$ are the corresponding outcomes of those variables. The conditional probability of $c_{n}$ can be computed using Bayesian theorem as follows:
%
%\begin{equation}
%P(C_{k}|x)=\frac{P(C_{k}) P(x|C_{k})}{P(x)}
%\label{eq:Naive_Bayes_Classifier}
%\end{equation}
%The above equation can also be formalized by using the following plain text.
%
%\begin{equation}
%Posterior=\frac{prior * likelihood}{evidence}
%\label{eq:}
%\end{equation}
%
%\subsubsection{Support Vector Machine}
%
%The Support Vector Machine (SVM) is supervised learning model that analyzes data and recognize patterns, especially designed for binary classification. SVM utilizes mathematical programming to directly model the decision boundary between classes. Given a set of training examples each of which belong to one of two categories, an SVM training algorithm builds a model that assigns new examples into one of the two categories.
%The operation of the SVM algorithm is based on finding the hyperplane that gives the largest minimum distance to the training examples. This distance receives the important name of margin within SVM’s theory. Therefore, the optimal separating hyperplane maximizes the margin of the training data.
%
%
%\subsubsection{Bayesian network}
%A Bayesian network is a graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph. Actually, Bayesian network is a directed acyclic graph whoch nodes represent variable in Bayesian sense. Nodes are unknown variable, hypothesis or observable quantity. Edges from one node to others represent conditional dependencies among them. Nodes with no edges mean they are conditionally independent. Each node in Bayesian network is associated with a probability function that takes input from its parent variables and gives the probability of the variable represented by the node. 
 %
%Let take a simple Bayesian network example such as rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet.
%
%\begin{figure}[h!]
  %\centering
    %\includegraphics[angle=0,height= 60mm,width=60mm]{images/literature/bayesian_network.eps}
		%\caption{The directed acyclic graph of a Bayesian network}
		%\label{Bayesian_network_graph}
%\end{figure}
%
%If the variables are G = Grass wet (yes/no), S = Sprinkler turned on (yes/no), and R = Raining (yes/no), the Bayesian model using conditional probability will be
%
%\begin{equation}
%P( R = T | G = T )=\frac{P(G = T,R = T)}{P(G = T)}=\frac{\sum_{S \epsilon {T,F}} P(G = T, S, R=T)}{ \sum_{S,R \epsilon {T,F}} P(G=T,S,R)}
%\label{eq:bayesian_equation}
%\end{equation} 
%
%\subsubsection{K-Nearest Neighbor}
%
%K-Nearest Neighbor (KNN) is a type of instance based learning for classifying objects based on closest training examples in the feature space. The training examples are mapped into multidimensional feature space which is partitioned into regions by class labels. A point the space is assigned to the class if it is the most frequent class label among the k nearest training samples. A Euclidean distance is used to compute the closeness to the samples. The number of neighbors can be set as a parameter or be selected considering the mean squared error for the training set. 

\subsection{Challenges in Software Defect Prediction}

Most of the SDP models use both statistical model and machine learning model for predicting the defect for a software. These models usually use public Dataset and others use private Dataset for predicting defects. Although, The SDP models help a lot the software development team, the problems that always arise and have significant impact in software defect prediction process are described below :


\subsubsection{Dataset Collection}

Every SDP model uses software data such as software code mertics, defect information etc. for predicting the software defect. From the literature in SDP, it is obvious that most of the models use public data source from the defect repository and very few models use private data for SDP. The most popular data sources for SDPs are Promise Repository \cite{promise12} and NASA \cite{nasa2007respository}. 

The SDP model using private Dataset can not be compared to other models because their Datasets cannot reached. 
To make any SDP model acceptable to others, it is always appreciated to use public data source for the defect prediction. So, software engineering research should public Dataset such as Promise Repository \cite{promise12} for building the prediction model.


 %and  however the number of SDP model using private data is also increasing. The majority of the SDP studies today are done on Java. 
%
%Since the large majority of SDP studies use repository data (i.e.,
%75\% from source code and 69\% from defect repositories), it is worth investigating the
%quality of the data stored in software repositories. As mentioned earlier, researchers
%have have already begun looking in this direction (e.g., [25, 41, 154, 217]). We believe that this line of research is very important for future SDP work and encourage
%researchers to further investigate what other types of data can be used to mitigate challenges related to noise in repository data.


\subsubsection{Model Selection}
There are lots of prediction model in literature used in SDP. some of those models are already described in section \ref{Ways_Of_the_Software_Defect_Prediction}. The model selection process for defect prediction is very much critical because its success depends on the selected model used for the prediction. Prior researches show that most of the SDP models use machine learning or statistical inferences such as Decision Tree, Linear Regression, Bayesian Network, Support Vector Machine, Logistic Regression etc. Before selecting a prediction model, it should be ensured that the prediction model should best fit the data and can explain the data accurately. 

\subsubsection{Data Analysis}
The success of software defect prediction process depends on the training process of the SDP model. Normally, the SDP model uses software code metrics and bug history as the training data. The software engineering data always contains some false information which mislead the training process. To provide the best data in the training process, many researches have been performed to find out best set of data. Some researches suggest to group the Dataset, so that the SDP model gets proper training Dataset. So, before applying SDP model to the Dataset, it is needed to ensure that the prediction model fit to the data.     


\section{Software Code Metrics}
\label{software_code_metrics_description}
The software metric or code metric is a quantitative measure of a software system. There are lots of software metrics, for example, method level, class level, component level, process level, Cyclomatic complexity etc.,  that have been used for defect prediction to improve the software quality. The software code metrics have multidimensional usage in software industry such as defect prediction, schedule and budget planning, cost estimation, quality testing, software debugging, software performance optimization etc. In this thesis, the well known Object Oriented and size metrics, have been used for defect prediction, are described below:

 %The methods level metrics are widely used for defect prediction for structured programming and Object oriented programming paradigm and The class level metrics are only used for Object oriented programs. All of the metrics properties are not significant for all cases. To analyze these, many reseaches have already been carried out to identify best set of metrics for defect prediction.

\subsection{Object Oriented Design Metrics}
\label{Code_Metrics_background_study}
When the object oriented software development paradigm became popular, a lot of researches have been conducted to find out the most important code metrics for the Object Oriented (OO) designed system. One of the most important OO code metrics is CK metrics, was proposed by Chidamber and Kemerer \cite{chidamber1994metrics}. In addition to CK metrics, there are also other metrics suite such as MOOD \cite{bansiya2002hierarchical}, QMOOD \cite{olague2007empirical}, L\&K \cite{abreu1994object} etc. Among all the OO design metrics, the CK metrics is the most influential code metrics and widely used to measure the quality of OO designed systems \cite{catal2009systematic,he2015empirical,okutan2014software}. The details description of the CK metrics are given below:

%The DIT will be the maximum length from the node to the root of the tree. 
\textbf{Weighted Methods per Class (WMC)}
The WMC is the sum of all methods that belong to a class \cite{chidamber1994metrics}. It indicates how much effort is required to develop and maintain a particular class. The low value of WMC points to greater polymorphism and the high value indicates the class is complex, difficult to maintain and reuse. The classes with high value of WMC is often re-factored into two or more classes to make the source codes simple and maintainable.
% the WMC value should be smaller than 40. 

\textbf{Depth of Inheritance Tree (DIT)}
The DIT simply counts the number of ancestors of a class \cite{chidamber1994metrics}. The high value of DIT means the class inherits greater number of methods which makes a class complicated and complex. To the Object Oriented programing (OOP) point of view, a class should maintain the single responsibility principle. When a class inherits a lot of classes, it is most likely that the class inherits a lot of methods which is the violation of the OOP principle. As a result, the designed system cannot utilize the benefits of the OOP principles. 

%The OO designed system's class hierarchy is illustrated in Figure \ref{dit_codeMetrics}. 
\begin{figure}[h!]
  \centering
    \includegraphics[angle=0,height= 60mm,width=70mm]{images/literature/Code_metricAnalysis.eps}
		\caption{The class hierarchy of a OO system}
		\label{dit_codeMetrics}
\end{figure}
From the Figure \ref{dit_codeMetrics}, the DIT values of different classes are given below:
\begin{itemize}
\item DIT ($C_{0}$) = 0
\item DIT ($C_{1}$) = 0
\item DIT ($C_{2}$) = 1
\item DIT ($C_{3}$) = 2
\item DIT ($C_{4}$) = 3
\item DIT ($C_{5}$) = 4
\end{itemize}

\textbf{Number of Children (NOC)}
The NOC simply counts the immediate sub-classes of a given class \cite{chidamber1994metrics}. It is also a measure of how many sub-classes are going to be inherited by the methods of a particular parent class. From the Figure \ref{dit_codeMetrics}, the NOC values of different classes are given below:

\begin{itemize}
\item NOC ($C_{0}$) = 1
\item NOC ($C_{1}$) = 1
\item NOC ($C_{2}$) = 1
\item NOC ($C_{3}$) = 2
\item NOC ($C_{4}$) = 3
\item NOC ($C_{5}$) = 4
\end{itemize}

The NOC value also gives the potential influence of a class on the system designing. The high value of NOC ensures the high usage of the inheritance and the greater possibility of the reusability in a system. Sometimes, it may also be a case of misuse of sub-classing and increase the system complexity that results more testing of the methods to ensure the quality of a software.

\textbf{Response for a Class (RFC)} 
The RFC metric is the total number of all methods that can be executed in response to a message received by a class \cite{chidamber1994metrics}. It is the sum of the methods of a class and all distinct methods that are invoked directly within the class methods. If a class invokes large number of methods in a response, the testing and debugging of the class becomes more complicated and  the tester requires a greater level of understanding to test the method. 

%The larger the number of methods that can be invoked from a class, the greater the complexity of the class. A worst case value for possible responses will assist in appropriate allocation of testing time.


\textbf{Coupling Between Objects (CBO)}
The CBO simply counts the number classes to which one single class is connected \cite{chidamber1994metrics}. A class can be coupled to other classes through parameters passing, shared variables, common database access or direct method calling etc. From the example in Figure \ref{CBO_codeMetrics}, the coupling value of these classes are listed below: 
\begin{figure}[h!]
  \centering
    \includegraphics[angle=0,height= 70mm,width=90mm]{images/literature/CBO.eps}
		\caption{The Class Coupling among different classes in a OO system}
		\label{CBO_codeMetrics}
\end{figure}

\begin{itemize}
	\item CBO(A)=2
	\item CBO(B)=2
	\item CBO(C)=3
	\item CBO(D)=1
\end{itemize}

The High value of CBO is always detrimental to modular design and prevents reuse of a classes. When a class has high CBO value, it means that the class has high dependency to the other classes. So, the corresponding class has high possibility of having defects than others and needs more testing compared to others. On the contrary, the low value of CBO indicates the simplicity of a class and it is easy to reuse in another application. 


\textbf{Lack of Cohesion of Method (LCOM)} 
The cohesion measures how well the methods in a class are connected to each others \cite{chidamber1994metrics}. It actually means the number of methods that do not share common fields like methods, properties etc. A class is called cohesive class if it performs only one function. So, the lack of cohesion means that the class performs more than one responsibility. From the OOP point of view, a class should perform only one responsibility. The LCOM value is used to determine whether the class violates the single responsibility principle or not. 

LCOM = 1 means that the class is cohesive class and LCOM $\geq$ 2 means that the class has more than one responsibility and should be split to make the design simple. High cohesion among the methods is always desirable because it helps to achieve encapsulation. Low cohesion indicates the inappropriate design and high complexity which leads to create fault prone software.   

%The metric calculates whether the fields in an object are used across the method and properties or whether there is a split. A split means that some methods access some fields and some access other fields. That means that there is no interaction among these methods and so they should be in different classes. 
\begin{figure}[h!]
  \centering
    \includegraphics[angle=0,width=70mm]{images/literature/lcom1.eps}
		\caption{A simple overview of lack of cohesion}
		\label{lcom1}
\end{figure}


In the Figure \ref{lcom1}, a class consists of methods A through E and variables x and y. A calls B and B accesses x. Both C and D access y. D calls E, but E does not access any variables. This class has 2 unrelated components. The class can be split into the following components such as {A, B, x} and {C, D, E, y}. So, it will be good to split the class into two classes. For this class, the LCOM value is 2. 
\begin{figure}[h!]
  \centering
    \includegraphics[angle=0,width=70mm]{images/literature/lcom2.eps}
		\caption{A simple overview of a cohesive class}
		\label{lcom2}
\end{figure}

In Figure \ref{lcom2}, Method C access x to increase cohesion. Now the class consists of a single component where each and every variables and methods are connected to each others. So the LCOM value for this class is 1. It is a cohesive class. 

\subsection{ Software size metrics}
Besides the CK metrics, the Number of Public methods and the Lines of Codes are popular code metrics, that have been employed for defect prediction. The short description of those are given below:

\textbf{Number of Public Method (NPM)}
The NPM is the sum of public methods in a class. It provides the number of methods that can accessible from outside of the class. The high value of NPM indicates that the class is highly coupled with other classes because public methods is a interface by which classes communicates to each others. The high value of NPM also shows that the corresponding class is too complex and has many responsibilities which complicates the design of a system.

\textbf{Lines of Code (LOC)}
The LOC simply counts the total number of lines from the source codes by avoiding pure whitespace and lines containing only comments. It measures the volume of the source codes. It can only be used to compare projects, built by the same programing language and the same coding standards. It is typically used to predict the amount of effort that will be required to develop a program, as well as to estimate programming productivity or maintainability once the software is produced.

% To change one is to change the volume of code. A better method to compare without regard to direct volume is to measure the complexity of the software. 

\subsection{Other Code Metrics}
Besides the above discussed code metrics, there are also other code metrics such as Halstead Metrics, McCabe metrics and Cyclomatic complexity etc. All of those code metrics are frequently used for defect prediction. The McCabe metrics is used to capture the structural complexity of the source codes \cite{mccabe1976complexity}. It includes cyclomatic complexity v(g), design complexity ev(g) and essential complexity iv(g). Cyclomatic complexity measures the number of linearly independent paths of a program execution which is widely used to understand the source codes complexity. The Halstead Code Metrics \cite{halstead1977elements} is another way to measure the complexity of a program's source code. It measures the complexity and difficulty of the source codes using the number of distinct operators ($n_{1}$), the number of distinct operands ($n_{2}$), the total number of operators ($N_{1}$) and the total number of operands ($N_{2}$). It can be used to calculate the program's length, difficulty, effort, bug and volume etc. 

\section{Conclusion}
In this chapter, the software defect, its prediction process, widely used defect prediction model and the software code metrics used in the defect prediction are described briefly.  
\chapter{Literature Review}
\section{Introduction}
\section{Code Metrics Selection}
%A software metric is a quantitative measure of a software system to define the quality of a software. There are lots of software metrics Such as method level, class level, component level, process level code metrics etc. The method level metrics is widely used in structured programming and Object oriented programming paradigm and the class level code metrics is only used for Object Oriented Programing (OOP) paradigm. This experiment deals with OO designed system, so, the class level metrics is matter of concern in this thesis.  Many researches have already been performed to find the best metric suite for OOP paradigm. This thesis only uses these premises to select best set of metrics in defect prediction. Some of these researches are described below: 
%Hence, this experiment has been conducted on OOP paradigm,
%There are lots of metrics suite in class level code metrics such as CK metrics, MOOD, QMOOD, L\&K metrics etc.
    
The code or software metric is a quantitative measure to define the quality of a software system. There are lots of software metrics such as method level, class level, component level, process level, cyclomatic complexity etc. 
These software metrics have multidimensional usage in the software industry such as schedule and budget planning, cost estimation, quality testing, software debugging, software performance optimization etc. Apart from these, it have been used for defect prediction to improve the software qualities. Among all the metrics, the method level metric is widely used in structured programming and Object Oriented Programing (OOP) paradigm and the class level code metric is only used for OOP paradigm. All of the above metrics' properties are not significant for defect prediction \cite{catal2009systematic,pai2007empirical,zhou2006empirical}. To analyze the importance of those metrics, many researches have already been carried out to identify the best set of metrics for defect prediction. Some of these researches are described below:
 
%It was the first attempt to measure the software code metrics by Thomas J. McCabe Sr. in 1976.It is a well known software metrics to measure the complexity of a source codes.
Cyclomatic complexity is the first attempt to measure the complexity of a software by Thomas J. McCabe Sr. in 1976\cite{mccabe1976complexity}. It computes the complexity of a program by measuring the linearly independent execution paths from the source codes. It draws the control flow graph of a program by considering each instruction as node and data flow among nodes as directed edges. The Cyclomatic complexity, ($M$) of a program can be calculated by using the following equation:  
	\[M = E − N + 2P,\] where,
E = the number of edges of the graph,
N = the number of nodes of the graph and 
P = the number of connected components.
It suggests that the cyclomatic complexity of a program should be smaller than 10 because the high cyclometic complexity means the high probability of having faults \cite{mccabe1976complexity}. It has been used to measure the quality of source codes. Besides this, it is also widely used for test case generation, defect prediction.

Halstead Code Metrics \cite{halstead1977elements} is another way to measure the complexity of a program's source code. It measures the complexity and difficulty of the source codes using the number of distinct operators ($n_{1}$), the number of distinct operands ($n_{2}$), the total number of operators ($N_{1}$) and the total number of operands ($N_{2}$). It uses $n_{1}$, $n_{2}$, $N_{1}$ and $N_{2}$ to calculate the following measures: 

\begin{itemize}
	\item Program vocabulary: $\eta$ = $\eta_1$ + $\eta_2$ \,
	\item Program length: N = $N_1$ +$N_2$ \,
	\item Calculated program length: \^{N} = $\eta_1 \log_2 \eta_1$ + $\eta_2 \log_2 \eta_2$ \, 
	\item Volume: V = N $\times$ $\log_2 \eta$ \,
	\item Difficulty : D = { $\eta_1 \over 2 $ } $\times$ { $N_2 \over \eta_2$ } \,
	\item Effort: E =  D $\times$ V \,
	\item Program’s execution time: T = {$E \over 18$}\,
	\item Delivered bug: B = {$V \over 3000$} \,
\end{itemize}


%Chidamber and Kemerer \cite{chidamber1994metrics} proposed the most important software code metrics for Object Oriented (OO) system to help the designers and managers by providing the inner design details of a OO system.

The CK metrics, proposed by Chidamber and Kemerer, to help the designers and managers by providing the inner design details of a Object Oriented (OO) system \cite{chidamber1994metrics}. These metrics are calculated by inspecting the relationship among different classes from a OO system. These are Weighted Methods per Class (WMC), Depth Inheritance Tree (DIT), Number of Children (NOC), Coupling between Object classes (CBO), Response for Class (RFC) and Lack of Cohesion in Methods (LCOM). These metrics are widely employed in fault prediction to improve the quality of a OO system. The details description of these metrics are given in section \ref{Code_Metrics_background_study}. 

To analyze the impact of the CK metrics in defect prediction \cite{chidamber1994metrics}, Basili et al. performed an experiments using logistic regression to explore the relationship between the CK metrics and the fault-proneness of the OO classes \cite{basili1996validation}. To perform the experiments, it collects eight software projects, developed by eight groups of students at University of Maryland using C++. The results show that the code metrics such as RFC, NOC, DIT are very significant in defect prediction. The CBO, WMC are significant specially in user interface (UI) level classes and the remaining metric, the LCOM seems to be significant at all cases.

%Due to the inability of traditional approaches in class level metrics, Bansiya et al. proposed a class level metrics, named as QMOOD, a hierarchical quality model dedicated to the assessment of Object Oriented Design (OOD) quality \cite{bansiya2002hierarchical}. It assigns OOD metrics to OOD properties such as number of classes to design size, direct class coupling to coupling, number of methods to complexity etc. It also links OOD properties to quality attributes such as reusability, flexibility, functionality etc. It also identifies the specific design properties for the object-oriented paradigm, for example, polymorphism, data abstraction, hierarchies. The main limitation of this paper is the unclear mapping from the components to the metrics. 

 %The paper also introduces a very interesting set of new object oriented metrics. But the major drawback is that QMOOD
%still relates design properties directly with metrics, in the same manner criteria are mapped to metrics in the FCM model. Thus, again design characteristics are related explicitly to “raw numbers” (i.e., metrics), while the rules and principles of good-design that determined the mapping remain implicit.

Gyimothy et al. performed a study to analyze how CK metrics can be employed for the fault-proneness detection of an open source software \cite{gyimothy2005empirical}. For this purpose, it uses statistical methods, for example, logistic and linear regression and machine learning algorithms such as decision tree and neural network. To perform the experiments, it collects an open source software, named as Mozilla and its bug report from Bugzilla database \cite{BugzillaforMozilla}. After that, it applies the statistical methods and the machine learning algorithms to the Mozilla. The results show that the CBO is the best metric and WMC, RFC, LOC are the most significant metrics, while DIT, LCOM are less significant and the NOC seems to be unimportant in defect prediction.

The usefulness of OO metrics such as CK metric \cite{chidamber1994metrics}, in fault prediction was also investigated by Zhou et al. \cite{zhou2006empirical}. It focuses on how accurately the six CK metrics can predict defects when taking the fault severity into account. Each of these metrics have been tested using logistic regression and three machine learning algorithms such as Naive Bayes, random forest and Nearest Neighbor with Generalization (NNge), on a public NASA dataset such as KC1. The findings of this experiments show that the CK metrics work well in low severity of defects rather than the high severity. It also shows that the CBO, WMC, RFC and LCOM metrics are statistically significant in both high or low fault severity. The DIT is not significant at any severity level while NOC is only significant at low fault severity. 
%The main limitation of this experiments is the accuracy of fault severity ratings in KC1. 

Pai et al. used Bayesian networks to analyze the effects of CK metrics \cite{chidamber1994metrics} on the number of defects and the defect proneness of a class \cite{pai2007empirical}. It uses KC1 project from the NASA metrics data repository. It builds a Bayesian network where parent nodes are the CK metrics \cite{chidamber1994metrics} and child nodes represent the fault content and fault proneness. After the model has been created, it uses  Spearman correlation analysis \cite{cohen2013applied} to check whether the variables of the CK metrics are independent or not. It founds that SLOC, CBO, WMC and RFC are the most significant metrics to determine fault content and fault proneness. It discovers that the correlation coefficients of these metrics such as SLOC, CBO, WMC, and RFC with fault content are 0.56, 0.52, 0.352, and 0.245 respectively. On the other hand, it also finds that both DIT and NOC are not significant and LCOM seems to be significant for determining fault content.
   
Catal investigated 90 software defect prediction papers published between 1990 and 2009 \cite{catal2009systematic}. He categorized those papers and reviewed each paper from the perspectives of metrics, learning algorithms and the data sets. According to this review, the method level metrics such as Halstead \cite{halstead1977elements} and McCabe \cite{mccabe1976complexity} matrics are most influential matrices in defect prediction and also suggests to use class level metrics for the OO programs. It also shows that the CK metrics is mostly used class level metrics in the defect prediction. 
 
Radjenovi´c et al. \cite{radjenovic2013software} classified 106 papers of the software defect prediction according to metrics and context properties. It concludes the proportions of object-oriented metrics, traditional source code metrics, and process metrics are $49\%$, $27\%$, and $24\%$, respectively. Among all of metrics, CK \cite{chidamber1994metrics} metrics are the most frequently used in defect prediction. The CK metrics has been reported to be more successful than traditional size and complexity metrics. 

Okutan et al. performed an experiment to identify the most effective set of matrices in the defect prediction \cite{okutan2014software}. For this purpose, it uses Bayesian networks to determine the probabilistic influential relationships among the software code metrics. It introduces two new code metrics such as number of developers (NOD) and lack of coding quality (LOCQ). An experiment has been conducted on the nine open source Promise Repository  datasets \cite{promise12}. The results show that RFC, LOC and LOCQ are the most significant and effective metrics and LCOM, WMC, CBO are less effective metrics in the defect prediction. In addition, it also shows that NOC and DIT are not effective metrics in defect prediction. 

Peng He et al. \cite{he2015empirical} performed an experiment on 34 release of 10 open source software projects to find out the most effective set of code metrics for the defect prediction. It also identifies the most suitable defect prediction model that works well in both within project and cross project. The proposed technique uses greedy step wise search algorithm by using forward or backward approach for finding the best of code metrics. It identifies the top k-code metrics that are CBO, LOC, RFC, LCOM, CE (Efferent Coupling), NPM, CBM, WMC etc. Among all the metrics, it lists CBO, LOC and LCOM as the most significant and influential metrics in defect prediction. It also shows that the success of any prediction model depends on the training data.

The above discussions on the software code metrics show the class level metrics, especially CK metrics suite, is the most popular and widely used metrics in the software defect prediction. Among the CK metrics, the CBO, RFC, LCOM, WMC are seem to be most significant and NOC and DIT are less significant metrics. The widely used size metrics LOC and NPM are also significant in defect prediction. Many researches also show that the other metrics such as CE and CBM have positive impact in defect prediction. So, To incorporate all the previously investigated experiments, the CK metrics along with CE and LOC, NPM should be used as selected code metrics for the defect prediction.     

 %CK metrics are not significant.to use class level metrics to predict defects. At the same time, numerous comparisons among different software metrics have also been made to select metric or combination of metrics that perform better in defect prediction.
\chapter{Package Based Clustering}

\section{Introduction}

Software defect prediction models use software code metrics and knowledge from previous projects to predict software defects. Early estimating the faultiness of a software can help practitioners to assess their current project status, as well as it reduces the software development cost. To predict defects before software testing process, many researches have been conducted different defect prediction models, for example Neural network, Naive bayes, Regression modeling and Decision tree etc. \cite{catal2009systematic}. Most of those models perform software defect prediction by considering the entire system. Due to variability in software data, prediction models considering the entire system do not always provide the desired results \cite{bettenburg2012think}. So, if software defects are predicted by partitioning the software into multiple clusters, it may produce better results than those \cite{scanniello2013class}.

Software clustering can be accomplished by using different clustering algorithms such as BorderFlow \cite{}, K-means \cite{}, subtractive \cite{} clustering etc. Those clustering algorithms use software code metrics, for example, Class level, Method level, process level etc. or source code dependencies, for example, class reference, to form clusters from the software. Although clustering algorithms help defect prediction models to improve the prediction results, all of those cannot always provide the best results because of the inefficiency of algorithms to group the software.

In recent years, several software defect prediction models have been developed to predict software defects. Menzies et al. \cite{menzies2013local,menzies2011local} proposed that learning from software clusters with similar characteristics is better than learning from the entire system, because it may falsify the data used by the prediction model. It performs clustering by using principle component analysis which only considers the code metrics and learning treatment from pairs of neighboring clusters. Giusepps Scaniello et al.\cite{scanniello2013class} proposed a defect prediction model using clustering where it considers step wise linear regression model to predict defects. It uses BorderFlow algorithm to form clusters among the related classes by using references between methods and attributes. Sidhu et al. \cite{sidhu2010subtractive} proposed a software fault prediction model which uses subtractive clustering algorithm and fuzzy inference system for early detection of faults. To predict the faults, Zimmermann et al.\cite{zimmermann2007predicting}  proposed the use of package or components for grouping software metrics because the likelihood to fail of a component or package is dependent on its problem domain.

In this dissertation, a new ways to group the source codes for defect prediction is proposed to improve the software defect prediction accuracy by using Package Based Clustering (PBC) rather than the entire system.
PBC groups the software, implemented by java, into a number of clusters using package information of each OO classes. To find clusters, PBC lists out all OO classes by using textual analysis of source codes. It then reads those classes to extract the package information to form clusters. If the number of OO classes of a cluster is smaller than the number software code metrics characteristics which are considered the explanatory variables used in the defect prediction model, it combines small clusters to make those enable to apply prediction model.

To validate the proposed clustering algorithm, an experiment has been conducted on an open source software named as JEdit 3.2 from promise repository \cite{promise12}. At the beginning of the experiment, the selected software is partitioned into multiple clusters using PBC algorithm. Then the linear regression model has been applied to each cluster to find out predicted defects. Finally, results of the PBC are compared with two prominent clustering schemas, named as Border Flow and K-means and the entire system, to show the importance and effectiveness of the proposed PBC. In this context, by considering the OO classes' relationships and similarities, PBC performs better than BorderFlow and K-means clustering algorithm to enhance the prediction accuracy.

\section{Related work}

The literature in defect prediction focused on predicting software defect by establishing relationships between software defects and code metrics. Existing software defect prediction techniques use different types of prediction models (e.g. statistical inference and machine learning model) and dataset (e.g. Promise repository and NASA dataset) to predict defects. Some prediction models predict defects considering the entire system and others use clustering of software to predict defects. 
 
%Catal et al. performed a study on software defect prediction with respect to metrics, method and dataset \cite{catal2009systematic}. It summarizes that most of the defect predictors use machine learning algorithms or statistical inferences to predict defects. It also shows that method and class level metrics are most dominant metrics in software defect prediction and suggested to perform more researches on component and package level defect predictions.
\subsection{Clustering Based Defect prediction}
The clustering based defect prediction model divides the whole software's dataset into multiple clusters for training the prediction model. The software engineering datasets always contain a lots of variabilities such as heterogeneity among the code metrics. These variabilities cause the poor fit of machine learning algorithms or statistical inferences to the dataset. If the variablities among the datasets can be minimized by clustering, it will increase the probability of fitting the data to the machine learning algorithms or statistical inferences.  Many researchers have already been carried out to group the software engineering dataset for predicting the defects. Some of these experiments are described below:
%The performance and accuracy of a defect prediction model depends on the training dataset. 
 %Software development is complicated, brainstorming and error prone task. In the large software development, The development teams work on concurrently on different parts of the software. As a result, there exists high probability of producing errors in the last product. If the software bugs or defects can be predicted, it will be beneficial for the project manager and other stakeholders. To resolve this problem, many researchers have already identified many defect prediction approaches for early estimating the defects in a system.

Schroter et al. proposed a defect prediction model that uses program's import dependencies to predict defect for a Object Oriented (OO) class \cite{schroter2006predicting}. For each OO file, it groups its imported classes and packages to form a cluster and maps the past failure history of these imported classes and packages to the selected OO class. It then predicts the failure-prone possibility of the OO class by using four prediction techniques based on linear regression model, Ridge regression, Regression tree and Support vector machine respectively. The proposed model has been implemented on $52$ eclipse plug-ins. The results show that the design and past failure history of a software can be used in defect prediction. It was the first experiment which tries to group the program's dependencies for predicting defects. Although it works well, the main drawbacks of this experiment is: it only considers import dependencies by ignoring the impact of other dependencies such as call dependencies, data dependencies etc. 


%Zimmermann et al.\cite{zimmermann2007predicting} proposed a technique to predict defects at the design time considering program dependencies. It is actually the extension of Schröter et al. \cite{schroter2006predicting} which considers only import dependencies while analyzing the dependencies among different parts of the software. On the other hand, this technique considers call dependencies, data dependencies, and Windows specific dependencies such as shared registry entries. This paper tries to find the answer of how dependencies predict post-release defects. To perform the experiments, it collects dependencies of binaries, such as executable files (COM, EXE, etc.) and dynamic-link files (DLL), for Windows Server $2003$. It then applies Support Vector Machine (SVM) to predict the post release defects at design time with precision ranged between $0.58$ and $0.73$. It concludes that the defect proneness of a software can be predicted by using the dependencies among all binaries. 
To resolve the above problems, Zimmermann et al. proposed a technique to predicts defect at the design time by considering call dependencies, data dependencies, and Windows specific dependencies such as shared registry entries \cite{zimmermann2007predicting}. It uses Support Vector Machine (SVM) to predict the post release defects at design time with precision ranged between $0.58$ and $0.73$. To perform the experiments, it collects the dependencies of all binaries such as executable files, for example, COM, EXE, etc. and dynamic-link files, for example, DLL, for Windows Server $2003$.  It concludes that the software's defect proneness can be predicted by using the dependencies among all binaries. These usage of codes' dependencies indicate the importance of using clustering technique in defect prediction. 

Tan et al. proposed a defect prediction method based on functional clustering of the program to improve the performance\cite{tan2011assessing}. For finding clusters, it uses Latent Semantic Indexing (LSI) to group the software into multiple clusters. It uses the linear regression and logistic regression for building the prediction models. It selects two-thirds of the dataset to train the prediction models and the remaining part for test. The prediction capability is justified by using Pearson and Spearman correlation coefficients of predictive and actual defects \cite{cohen2013applied}. To assess the effectiveness of the proposed model, an experiment has been conducted on a software built by Java with a high fault probability. The results show that the predictive model built on clustering performs better than the class based models in terms of precision and recall. It is another important hypothesis to use the clustering technique in defect prediction.
%However, the used clustering technique was not automatic.

The usage of the subtractive clustering algorithm and the fuzzy inference system for early detection of faults was proposed by Sidhu et al. \cite{sidhu2010subtractive}. This approach has been tested with defect datasets of NASA software projects named as PC1 and CM1. It uses the combined model of requirements metrics and code based metrics from the dataset. Results show that the accuracy of this model is better than other models considering accuracy, mean square error and root mean square error. Although this approach performs well, there is no defined rules to find the sufficient number of clusters using subtractive clustering algorithm and when to stop executing the algorithm. The prediction model's accuracy shows more researches are needed to perform on source codes' clustering for defect prediction. 

%Sidhu et al. \cite{sidhu2010subtractive} proposed a software fault prediction model which uses the subtractive clustering algorithm and the fuzzy inference system for early detection of faults. This approach had been tested with defect datasets of NASA software projects named as PC1 and CM1. It used the combined model of requirements metrics and code based metrics from the dataset. Results showed that the accuracy of this model is better than other models in accuracy, mean square error and root mean square error. Though this approach performs well but the most unclear things in this algorithm are: how to find sufficient number of clusters using subtractive clustering algorithm or when to stop executing it.

 
To resolve the variabilities among the dataset, Menzies et al. proposed a software defect prediction model that learns from software clusters with similar characteristics \cite{menzies2013local,menzies2011local}. It shows learning from software clusters is better than learning from the entire system because it may falsify the data used by the prediction model. It performs clustering by using Principle Component Analysis (PCA) technique that considers only the code metrics and learning treatment using pairs of neighboring clusters. It also generates rules to reduce the number of defects from the local learning but there also exists the conclusion instability. It advises that empirical software engineering should focus on ways to find the best local lessons for groups of related projects. It also shows that global context is often obsolete for particular local contexts in defect prediction. This premise also shows the importance of using the clustering in defect prediction, so that the prediction model gets the accurate dataset for learning.

Bettenburg et al. \cite{bettenburg2012think} proposed three kinds of predictors; those were $(i)$ global models which trains using the entire dataset; $(ii)$ local models which trains using the subsets of the dataset, and $(iii)$ multivariate adaptive regression which splines a global model with local consideration. The third model is the hybrid between global and local models. The proposed three kinds of predictors use linear regression as prediction model. To perform experiments, it collects the dataset from Promise Repository \cite{promise12} and then it applies Correlation Analysis (CA) and Variance Inflation (VI) factors analysis on the dataset to find out the potential multi-collinearity between the source code metrics. In the case of Local model, dataset are partitioned into regions by a clustering algorithm, named as MCLUST, based on software code metrics. To avoid over-fitting in the Global and Local models, the appropriate subset of the independent variables are selected by using Bayesian Information Criterion (BIC). It solves the problems of over-fitting by defining penalty term for each prediction variable entering into the model. Finally it uses $10$-fold cross validation to get more stable and robust results. Results show that the local model is better than the global model and the global model with local consideration outperforms both the global and local models in all cases. This is also another implication of using clustering in the defect prediction. 
 
Scaniello et al. \cite{scanniello2013class} proposed a defect prediction model which predicts defects using step wise linear regression (SWLR) that uses clustering of the source codes rather than the entire system. It considers references between methods and attributes to form clusters among the related classes using BorderFlow algorithm \cite{ngomo2010low}. The BorderFlow clustering algorithm performs clustering by maximizing the flow from the border to center and minimizing the flow from border to outside of the cluster. Then it applies the SWLR model on each cluster and produces better results than other models that perform prediction considering the entire system. It focuses on clustering using source code whereas Menzies et al. \cite{menzies2013local,menzies2011local} focuses on clustering using code metrics. It forms clusters considering only related classes which means it only uses coupling information among the classes to form the clusters. So, the other code metrics' impacts are needed to analyze for defect prediction.  

In a nutshell, a general overview of defect prediction using clustering emphasizes to group the software source codes by applying different clustering approaches to train the prediction model more perfectly. All of the above discussed clustering algorithms such as BorderFlow \cite{scanniello2013class}, LSI \cite{tan2011assessing}, Subtractive clustering algorithm \cite{sidhu2010subtractive} use software code metrics, source code dependencies or code similarities etc. to group the source codes. Some approaches use PCA to reduce the dimension of the dataset before applying the different clustering algorithms \cite{menzies2011local,menzies2013local,sidhu2010subtractive}. None of those methods work perfectly in all Promise Repository's datasets \cite{promise12}. So, further researches are needed to perform on clustering of source codes for defect prediction.


%To perform different clustering algorithms on the dataset, the dimensions of the dataset are needed to reduce to fit the data for those clustering algorithms. Hence, Software code metrics directly relate to the defect., so the dimension of the dataset should be reduced based on the impact of these dimension to software defect.  

%All of the above discussed clustering algorithms such as BorderFlow, LSI, Subtractive clustering algorithm use either software code metrics, source code dependencies or code similarities etc. to group the source codes. Some approaches use PCA to reduce the dimension of dataset before applying the different clustering algorithms. In the defect prediction, 

 %Hence, the prediction models' accuracy totally depends on its training data, so the clustering approach to group the dataset for predicting defects will be helpful to improve the performance of the models.  


%To predict defects, all of the above discussed clustering algorithms such as BorderFlow, LSI, Subtractive clustering algorithm use either software code metrics, source code dependencies or code similarities etc. to group the source codes. None of those methods consider the code relationships and similarities together to group the software into clusters. Some approaches use PCA  However, the combined usage of the code relationships and similarities in clustering may enhance the software defect prediction accuracy.
\subsection{Other Defect Prediction Model }

A universal defect prediction model that is built from the entire set of diverse projects Zhang et al. proposed a defect prediction model 

Fenton et al. suggest to use Bayesian networks for defect, quality, and risk prediction of software systems \cite{fenton2002software}. They use the Bayesian network shown in Fig.2to model the influential relationships among target variable “defects detected” (DD) and the information variables “test effectiveness” (TE) and “defects present” (DP). In this model, DP models the number of bugs/defects found during testing. TE gives the efficiency of testing activities and DD gives the number of defects delivered to the maintenance phase. For discretization, they assign two very simple states to each variable namely low and high. Using the Bayesian network model, Fenton et al. show how Bayesian networks provide accurate results for software quality and risk management in a range of real world projects. They conclude that Bayesian networks can be used to model the causal influences in a software development project and the network model can be used to ask “what if?” questions under circumstances when
some process underperforms.

Turhan et al. proposed a defect prediction model using Naïve bayes technique that uses weighted importance of all features instead of treating equal importance\cite{turhan2007software}. It collects the software metrics and defect information from NASA repository \cite{nasa2007respository}. It uses three heuristics to estimate the weights of the features according to their importance. After that it applies weighted Naive Bayes and the standard Naive Bayes on the collected data. The results show that the proposed approach produces statistically better results for the defect prediction.  

Bibi et al. proposed a defect prediction model using Regression via Classification (RvC) to estimate the number of software defects in a project \cite{bibi2006software,bibi2008regression}. the regression models measures the number of faults and classification model identifies the fault-proneness of a class. So, this approach does not only classify a object into defective and non-defective rather it also outputs associated values. RvC has been experimented on two dataset such as pekka dataset and ISBSG dataset \cite{isbsg} using 10-fold cross validation. The results show that RvC gets better regression error than the standard regression error on both dataset.    


\section{Existing clustering algorithms}

For software defect prediction, many prominent clustering algorithms have been used to group software into multiple clusters. To validate the new clustering algorithm, the comparison needs to be accomplished among the proposed and existing clustering techniques. In this section, the existing clustering algorithms that are considered to compare results with the proposed clustering algorithm are described.

\subsection{BorderFlow}
BorderFlow clustering algorithm is successfully implemented in \cite{scanniello2011clustering,scanniello2013class} to group the software into multiple clusters. It treats the whole software as a collection of nodes to represent the whole software as a graph. It maximizes the flow from the border of each cluster to the nodes within the cluster, while minimizing the flow from cluster to the nodes outside.  In this context, the goal of this algorithm is to find groups of tightly coupled classes which are likely to implement a set of related features.

Let, a cluster X, is a subset of V, $b(X)$ is the set of border nodes of X, and $n(X)$ is a function used to identify the set of direct neighbors of X. $\Omega$ is a function that assigns the total number of the edges (i.e., dependencies) from a subset to another subsets using the \emph{equation (\ref{eq:function})}. Then borderFlow ratio can be measured by using the equation \emph{(\ref{eq:ration})}.

\begin{equation}
\label{eq:function}
\Omega(X,Y)=\sum e(c_{i},c_{j})| c_{i} \varepsilon X and c_{j} \varepsilon Y
\end{equation}
\begin{equation}
\label{eq:ration}
 F(x)=\frac{\Omega(b(X),X)}{\Omega(b(X),n(X))}
\end{equation}
 
To find group of related classes, this algorithm iteratively selects nodes from $n(X)$ and inserts  nodes in $X$ until $F(X)$ is maximized. The iterative selection of nodes ends when $n(X)$ equals to $0$ for each set of nodes.

\subsection{K-means}
K-means clustering algorithm divides a dataset into k clusters using an objective function called Residual Sum of Squares (RSS) \cite{July2014Online}. The RSS function is measured by using \emph{equation \ref{eq:k-means}}. If $\mid $x$_{i}^j-c_{j}\mid^2$ is a chosen distance measure between a data point $x_{i}^j$ and the cluster center $c_{j}$, the K-means minimizes the distance from $n$ data points to their respective cluster centers by using the \emph{equation (\ref{eq:k-means})}. 

\begin{equation}
\label{eq:k-means}
J=\sum\limits_{j=0}^k\sum \limits_{i=0}^n \mid x_{i}^j-c_{j}\mid^2
\end{equation}

This algorithm iteratively runs and computes RSS value to find clusters. In each iteration, it moves the cluster centers to minimize RSS value. This process continues until cluster centers do not move anymore.

\section{Simple overview of the Java Project}
As the experiment is based on the OO software built by Java, so this section provides the description of Java project hierarchy. Normally, a Java project consists of some Packages, classes, interfaces etc. For more clarification, the inner details of Package and Class are given below: 
 
\subsection{Package}
Packages in Java are used to organize source code files and prevent namespace conflicts.
 %To create a package is a simply easy by adding a package command in the beginning of Java source code file. Any classes declared within this file will belong to that package.
The package command declares a place where the classes will be stored. If package is not declared for a class, the class file will be stored in default package. This is the general form of the package statement:

Package pkg;

Here, pkg is the name of the package. For example, the following statement create a package named myPackage.
 
Package myPackage; 

Java uses file system directory just like a computer’s file system directory. More than one file can have same package name. The source file that are declared must be stored in a directory named the package name that is declared in the beginning of the file. 
In the case of nested package, a package may contain inner package. In that case the package naming structure can be expressed as follows 
Package pkg1[.pkg2[.pkg3]]; 
A short example of Java Package is given below:

\begin{lstlisting}
Package myPackage;
	class Calculator{
		int numberA, numberB;
			
		int Sum(int numberA, int numberB) {
			return numberA+ number;
		}
		
		Int multiply(int numberA, int numberB) ) {
			return numberA* number;
		}
	}

\end{lstlisting}
\begin{lstlisting}
// Hello.java
import javax.swing.JApplet;
import java.awt.Graphics;

public class Hello extends JApplet {
    public void paintComponent(Graphics g) {
        g.drawString("Hello, world!", 65, 95);
    }    
}
\end{lstlisting}

\subsection{Class}
A class is building block of all functionality in the object oriented concept. When a class is declared, it actually creates a new data type which is then used to create object of that type. Thus, a class is a template for an object, and an object is an instance of a class. 


When a class is declared, we need to declare its functionalities by specifying the data. A class may contain only data or only some functionalities, but most real world classes contain both. A class is declared by use of the class keyword. The classes that have been used up to this point are actually very limited examples of its complete form. Classes can get much more complex. The general form of a class definition is shown here:

\begin{lstlisting}

class classname {
type instance-variable1;
type instance-variable2;
// ...
type instance-variableN;
type methodname1(parameter-list) {
// body of method
}
type methodname2(parameter-list) {
// body of method
Chapter 6: Introducing Classes 131
THE JAVA LANGUAGE
}
// ...
type methodnameN(parameter-list) {
// body of method
}
}

\end{lstlisting}

The data, or variables, defined within a class are called instance variables because each instance of the class contains its own copy of these variables. Thus, the data for one object is separate and unique from the data for another. We. The code is contained within methods. The methods and variables defined within a class are called members of the class. The instance variable are often used by the method and it decides how the instance variable will be used.  

\section{Proposed Package Based Clustering (PBC) algorithm}

Existing clustering techniques consider a number of source code characteristics, for example, source code dependencies or similarity, LSI, code metrics, lexical similarity to group the software into clusters. To the best of our knowledge, no such technique yet considers both code relationship and similarity to group the source codes. In this paper, a new Package Based Clustering (PBC) algorithm is proposed to group the software. 

A package is a group of related types, for example, classes, interfaces, enumerations and annotations, which provides access protection to its elements. Programmers can define their own packages to resolve naming conflicts, to prevent access, to group related and similar OO classes together. Some of the existing packages in Java are $java.lang.$ and $java.io$ etc. A Java project hierarchy to represent project, packages and classes is depicted in Figure \ref{JavaStructure}.

\begin{figure}[h!]
\centering
      \includegraphics[angle=0,width=70mm, height=60mm]{javaheirarchy2.eps}
			\caption{Java project hierarchy}
			\label{JavaStructure}
\end{figure}


\subsection{Software Defect Prediction Model}
To implement the proposed PBC, a prediction model is required. We consider the linear regression model \cite{draper1981applied} to predict defects as it has been successfully implemented in \cite{tan2011assessing,basili1996validation}. The graphical representation of the proposed defect prediction model is depicted in {Figure \ref{defectPModel}. It explores the relationship between a dependent variable such as software defects and one or more independent variables such as WMC, DIT, NOC etc.(see Table \ref{ExplanatoryVariable}) providing with a model by a linear \textit{equation (\ref{eq:defectPModel}}) 
\begin{equation}
\label{eq:defectPModel}
 Y=b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{3}+...+b_{n}x_{n}+c
\end{equation}}
Where $Y$ is a dependent variable, $x_{1}...x_{n}$, are the independent variables, $b_{i}$ is the coefficient that represents the amount response variable $Y$ which changes when explanatory variables $x_{i}$ changes 1 unit and $c$ is the intercept.

\begin{figure}[h!]
\center
      \includegraphics[angle=0,width=90mm, height=50mm]{model3.eps}
		\caption{Software defect prediction model using Package Based Clustering (PBC)}
		\label{defectPModel}	
\end{figure}



\subsection{Selected Explanatory variables}
Software defect prediction models relate the code metrics to defect proneness of a software. Hence PBC adopts some selected explanatory variables, which are the same as Basili et al. \cite{basili1996validation}, identified eight significant object oriented code metrics, listed in Table \ref{defectPModel}. The first six metrics are proposed by Chidamber and Kemerer, popularly known as CK metrics \cite{chidamber1994metrics} and the next two are well known size metrics. In this paper, those eight code metrics are considered as explanatory variables for predicting defects (used in \textit{equation \ref{eq:defectPModel}}).

\begin{center}
\captionof{table}{Selected Explanatory variables}

\label{ExplanatoryVariable}
    \begin{tabular}{| p{6cm}  |p{9cm}|}
    \hline
    Weighted Methods per classes (WMC) & It is the sum of all methods in a class. \\ \hline
    Depth of Inheritance Tree (DIT) &  It is the length of the longest path from a given class to the root class in the inheritance hierarchy. \\ \hline
    Number of Children (NOC)  & It simply measures the number of immediate descendants of a class. \\ \hline
    Coupling between object classes (CBO) & It counts the number of other classes to which a given class is coupled. \\
    \hline
		Response for classes (RFC) & The RFC metric measures the number of different methods that can be executed when an object of that class receives a message.\\ \hline
		Lack of Cohesion in Methods (LCOM) & It is the measure of counting the total number of methods in a class that are not related through the sharing of some of the class fields.\\ \hline
		Number of Public Method (NPM) & The NPM metric simply counts all the methods in a class that are declared as public.\\ \hline
		Lines of Codes (LOC)& It simply counts the total number of instructions in a class. \\ \hline 
    \end{tabular}
	\end{center}


\subsection{PBC Clustering Technique with Joint Cluster}

In the Java programing convention, a package is a namespace that organizes a set of related and similar classes and interfaces. Conceptually, packages are similar to different folders in a computer. A package allows a developer to group classes (and interfaces) together. These classes are related in some way that they might perform a specific set of tasks. As our goal is to group related and similar classes, so clustering using PBC, described in  Algorithm \ref{PackageLevel}, can easily meet our purpose.

The proposed PBC algorithm groups a software into multiple clusters using related and similar OO classes. The functionality of PBC can be classified as OO class identification, Cluster formation, Cluster validation.

\subsubsection{OO class identification}

At the beginning of the clustering process, the proposed algorithm lists down all files from a software project and then it identifies potential files that will be considered for constructing clusters.  In our context, the software project is built using Java, so the proposed algorithm performs searching by considering the file extension with $.java$. 
The overall OO class identification process for PBC algorithm is illustrated below:   
\begin{algorithm}
%\caption{OO class identification}
\label{OO_class_identification}
\begin{algorithmic}[1]
%\REQUIRE  $ PackageName = 0$, $ ProgramClassList = 0$

	\FOR{\textbf{each} $file$}
		\IF{$fileNameExtention = .Java$}
				\STATE Add file to $programClassList$
		\ENDIF
	\ENDFOR

\end{algorithmic}
\end{algorithm}


\subsubsection{ Package Identification \&  Cluster formation}

After the identification of the OO classes, the PBC finds out the Package Name of each class. To identify package information of a Java file, it reads the file and retrieve the package name by matching the pattern structure, for example, $package$ $packageName;$. At the end of the successfully identification of the package name, it groups the files into multiple clusters based on the package name. For each distinct package name, it creates an array to store the file's name those are under the same package. if package name already considered, it adds OO class to the existing array of that package otherwise it creates array for the new package to add it's OO class.

The Package Identification and Cluster formation process is performed by using followings steps.  
\begin{algorithm}
%\caption{Package Identification and Cluster formation}
\label{Package_Identification_and_Cluster_formation}
\begin{algorithmic}[1]
%\REQUIRE $PackageContainer = 0$, $ PackageName = 0$, $ ProgramClassList = 0$, $NumberOfVaribale = 0$, $PackageSize = 0$

\FOR{\textbf{each} $programClassList$ }
			\STATE $Read Program File$ 
			\STATE $Search Each File For PackageName $  
			\IF{$packageName Contains In  PackageContainer$}
				\STATE $Add File To packageName$
			\ELSE{  }
			\STATE $Add New PackageName To PackageContainer$
			\ENDIF
	\ENDFOR
	

\end{algorithmic}
\end{algorithm}
In a word, the PBC finds out the Package Name of each class and lists all package name from the source codes as shown in line $1-2$. If package name already considered as cluster, it adds OO class to the existing package using line $4-5$ otherwise it creates a new cluster to add OO class as shown in line $6-7$.     


\subsubsection{Cluster validation} Packages may contain different number of classes. Some packages may have a lot of classes and other may have only few classes. Our software defect prediction model uses eight explanatory variables, listed in Table \ref{ExplanatoryVariable}. So if number of classes in a package is less than the explanatory variables used in the prediction model, it would fail to predict defects. In this case, to make defect prediction model successful, small packages are combined to form a joint cluster applying the following lines. 

\begin{algorithm}
%\caption{Package based Clustering (PBC)}
\label{PackageLevel}
\begin{algorithmic}[1]
%\REQUIRE $PackageContainer = 0$, $ ProgramClassList = 0$, $NumberOfVaribale = 0$, $PackageSize = 0$
	
	\FOR {\textbf{each} $Package$ in $packageContainer$}
		\IF{ $NumberOfClassInPackage$ \textless $NumberOfVaribale$}
			\STATE $Add To Joint Cluster$
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}

In a nutshell, the PBC algorithm groups a software project based on package information as package is a collection of similar and related classes. The whole PBC algorithm is illustrated in the Algorithm \ref{PackageLevel}.

\begin{algorithm}
\caption{Package based Clustering with Joint Cluster(PBC)}
\label{PackageLevel}
\begin{algorithmic}[1]
\REQUIRE $PackageContainer = 0$, $ PackageName = 0$, $ ProgramClassList = 0$, $NumberOfVaribale = 0$, $PackageSize = 0$

	\FOR{\textbf{each} $file$}
		\IF{$fileNameExtention = .Java$}
				\STATE Add file to $programClassList$
		\ENDIF
	\ENDFOR
\FOR{\textbf{each} $programClassList$ }
			\STATE $Read Program File$ 
			\STATE $Search Each File For PackageName $  
			\IF{$packageName Contains In  PackageContainer$}
				\STATE $Add File To packageName$
			\ELSE{  }
			\STATE{$Add New PackageName To PackageContainer$}
			\ENDIF
	\ENDFOR
	
	\FOR {\textbf{each} $Package$ in $packageContainer$}
		\IF{ $NumberOfClassInPackage$ \textless $NumberOfVaribale$}
			\STATE $Add To Joint Cluster$
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}
 

\section{Experimental Results and Analysis }

In this section, the experimental environment setup and the results analysis of the PBC along with two prominent clustering algorithms are analyzed. Finally, the effectiveness of the proposed PBC is measured by taking the mean, variance and standard deviation of absolute residuals. 

To perform data analysis, a tool based on the above description of the proposed technique was implemented.The prominent clustering algorithms such as BorderFlow and K-means was implemented by using Java and R language respectively. The linear regression model was built by using the R statistical environment. An open source software named as JEdit 3.2 was collected from promise repository \cite{promise12} to use in the experiment.

To perform the experiment, three clustering algorithms such as BorderFlow, K-means and the proposed PBC algorithm was applied on the Dataset JEdit 3.2. These three clustering algorithms divided the selected software into multiple clusters using their approach. The BorderFlow clustering algorithm was implemented as mentioned in \cite{scanniello2013class}. To find out the number of clusters using k-means, the center (n) value was iteratively changed to minimize the RSS. The proposed PBC was implemented as described in Algorithm \ref{PackageLevel} by using Java to classify all OO classes into multiple clusters.

%
%We implement three clustering algorithms such as BorderFlow, K-means and the proposed PBC algorithm on the dataset JEdit 3.2. The BorderFlow clustering algorithm is implemented as mentioned in \cite{scanniello2013class}. To find out the number of clusters using k-means, we iteratively change the center value (n) to minimize the RSS. The proposed PBC described in Algorithm \ref{PackageLevel}, is implemented by using Java. It takes a software project implemented by using Java, then it identifies all OO classes. It reads all OO classes and searches for package name in the OO class. After finding package name from the OO class, it classifies all OO classes into multiple clusters.

To analyze the quality of the prediction model obtained with the Linear Regression (see \textit{equation (\ref{eq:defectPModel})}), the k-fold cross validation was used. It is widely used to assess how accurately a predictive model will perform. It splitted each round dataset into training and test sets. The training set was used to train the prediction model and test set was used to assess how accurate the model can predict. 
 
To assess the quality of clustering algorithms, the Linear Regression model was applied on each cluster, obtained by using different clustering algorithms. It predicted the number of defects that contain in a OO class. It used Pearson and Spearman correlation coefficient to calculate the residual value of each OO class by taking the difference of predicted defect and actual defect. This process was conducted $50$ times to get stable residual value. The mean, variance and standard deviation of absolute residuals were then calculated to compare different clustering algorithms.


\begin{center}
\centering
\captionof{table}{The list of clusters obtained from different clustering algorithms }
 \label{tab:title} 
    \begin{tabular}{ | p{4cm} | p{4cm} | p{4cm} |}
			\hline
					Algorithm & Cluster & Joint cluster \\ \hline
					Entire System &	1 &	0\\ \hline
					K-means &	4	& 0\\ \hline
					BorderFlow	& 3 &	1\\ \hline
					Package Based &	7 &	1\\ 
			\hline
    \end{tabular}
	\end{center}
\newpage

Table \ref{tab:title} puts in a nutshell the experimental clustering results obtained from the experiments. The BorderFlow clustering algorithm finds $3$ clusters with $1$ joint cluster. The k-means finds $4$ clusters from the dataset with no joint cluster. The proposed PBC clustering algorithm finds $7$ clusters with $1$ joint cluster from the selected dataset.

\begin{center}
\centering
\captionof{table}{The mean, Variance and Standard deviation of absolute residual} \label{results} 
    \begin{tabular}{ | p{3cm} | p{3cm} | p{3cm} |p{3cm}|}
			\hline
				Algorithm &	Mean	& Variance &	Standard deviation \\ \hline
				Entire System&	1.248088 &	2.947011 &	1.716686\\ \hline
				K-means	& 1.058155 &	2.627515 &	1.620961 \\ \hline
				BorderFlow &	0.8904873	& 2.054801	& 1.433458 \\ \hline
				Package Based &	0.3404931 &	0.4378411 &	0.6616956\\ 
			\hline
    \end{tabular}
\end{center}

Table \ref{results} summarizes the mean, variance and standard deviation of absolute residuals obtained in the experiments. From the regression model, it is well established that the better the prediction model is, the smaller residual value will be \cite{draper1981applied}. Hence the prediction model's success depends on clustering algorithms, so clustering algorithm can improve the accuracy of a prediction model by grouping software properly. From the TABLE \ref{results}, It is obvious that the prediction model predicts the minimum value of mean, variance and standard deviation of absolute residual for the PBC clustering algorithm. So it is obvious that software defect prediction based on package based clustering algorithm is better than the prediction model considering the BorderFlow, K-means and entire system.
 
 \begin{figure}[ht!]
  \centering
    \includegraphics[angle=0,width=90mm]{standard_deviation.eps}
		\caption{Comparison of the standard deviation of absolute residual}
		\label{StandardDeviation}
\end{figure}


\begin{figure}[h!]
\centering
     \includegraphics[angle=0,width=90mm]{mean.eps}
		 \caption{Comparison of the mean of absolute residual}
		\label{MeanAbsoluteResidual}
\end{figure}

%Figure \ref{StandardDeviation} demonstrates the comparison of the Standard Deviation (SD) of absolute residuals. The SD refers the dispersion of sample value from mean. It is calculated to analyze how much better the calculated mean is. Comparing SD values obtained by using different clustering algorithms, Figure \ref{StandardDeviation} shows that the SD value is minimum for PBC clustering algorithm because prediction model performs better using this algorithm. It means that the dispersion of residual values are least for PBC. So, PBC outperforms than other clustering algorithms in defect prediction.   
%
%Figure \ref{MeanAbsoluteResidual} illustrates the comparison of mean values of the proposed PBC clustering algorithm along with others, for example, BorderFlow, K-means and the Entire system. It visualizes the importance of the PBC algorithm on defect predictions. After comparing mean value of PBC, BorderFlow, k-means and the Entire system, we concludes that defect prediction model considering PBC can significantly minimize the residual values because defect prediction model using PBC can accurately predict defects. In this context, prediction model using the proposed PBC minimizes the residual value $54\%$ than the prediction model using BorderFlow. In case of K-means clustering algorithm and the entire system, the PBC also performs $71\%$ and $90\%$ better to reduce the residual value. 

Figure \ref{StandardDeviation} demonstrates the comparison of the Standard Deviation (SD) of absolute residuals. The SD refers the dispersion of sample value from mean. It is calculated to analyze how much better the calculated mean is. Comparing SD values obtained by using different clustering algorithms, Fig. \ref{StandardDeviation} shows that the SD value is minimum for PBC clustering algorithm because prediction model performs better using this algorithm. It means that the dispersion of residual values are least for PBC. So, PBC outperforms other clustering algorithms in defect prediction.   

Figure \ref{MeanAbsoluteResidual} illustrates the comparison of mean values of the proposed PBC clustering algorithm along with others, for example, BorderFlow, K-means and the Entire system. It visualizes the importance of the PBC algorithm on defect predictions. After comparing mean value of PBC, BorderFlow, k-means and the Entire system, the results conclude that defect prediction model considering PBC can significantly minimize the residual values because defect prediction model using PBC can accurately predict defects. In this context, prediction model using the proposed PBC minimizes the residual value $54\%$ than the prediction model using BorderFlow. In case of K-means clustering algorithm and the entire system, the PBC also performs $71\%$ and $90\%$ better than those to reduce the residual value. 


\section{Conclusion}

We presented and experimented a new clustering technique for defect prediction using software clustering. The proposed clustering technique named as PBC is based on the related and similar OO classes that form packages in java programing convention. It uses textual analysis on source codes to identify OO classes from a software project and lists out those files. To form clusters, it extract the package information from each OO class by searching the package name. In special case, if the number of OO classes of a cluster is smaller than the number of explanatory variable used in the prediction model, it combines small clusters to make those enable to apply prediction model. Finally the linear regression model considering PBC is conducted on JEdit $3.2$. The experimental results show that the software defect prediction using the proposed PBC outperforms the prediction models considering BorderFlow, K-means and the Entire system because PBS uses source code similarities and relationships to group the software. In this context, the prediction model considering PBS is  $54\%$, $71\%$, $90\%$ better than the prediction models built on BorderFlow, k-means and the entire system respectively. 

As future work, to validate the observations and PBC algorithm, the dataset used by the prediction model needs to be extended and the research can also be extended to measure the quality of a software using different code metrics.  

\chapter{Code Metrics Similarity Analysis using Dimension Reduction technique}
\section{Introduction}

Source codes similarity analysis to group the software into multiple clusters can improve the performance and accuracy of a SDP model. The effectiveness of a SDP model depends on the learning procedure because the better learning increases the prediction accuracy of a SDP model. Usually, the software engineering Dataset are multidimensional and always contains a lots of variabilities that always hinders the learning procedure. To minimize these variabilities, multiple clustering algorithms can be applied on the Dataset to group these into multiple clusters. Due to the multidimensionality of the Dataset, clustering algorithms cannot perform properly and sometimes it seems to be impossible to group these Dataset into multiple clusters. So, if it is possible to minimize the dimension of the software engineering Dataset using the dimension reduction approach, it will definitely help the clustering algorithms to group the Dataset based on their similarity. 




%SDP models use software code metrics and defect information from the defect directory such as Mozilla defect directory \cite{BugzillaforMozilla} to predict defects. The effectiveness of a SDP model depends on the learning procedure of SDP model. The better learning can make the SDP model to predict software defect more accurately. Usually, the software engineering Dataset contains a lots of variabilities among the Dataset. These variablities always hinders the learning procedure. So, if it is possible to group the software engineering Dataset based on their similarities, it will definitely help the SDP model to learn better from the existing knowledge.

Due to the multidimensionality of the software engineering Dataset, many prominent clustering algorithms such as DBSCAN, K-means cannot divide these Dataset into clusters. To make enable these Dataset for these clustering approaches, the dimensions of the Dataset need to be reduced. If the Dimension of the Dataset is reduced based on the impact of independent variables to the dependent variable, the new Dataset gets closer value for similar objects. So, the clustering algorithm can perform better in dimension reduced Dataset to group these into multiple clusters based on their similarity. 

%The dimension reduction algorithms such as PCA, FA can be used to reduce the dimension of the Dataset. For the software engineering Dataset,

%The Dimension reduction process of the Software engineering Dataset is critical and it is always needed to ensure that dimensional reduced Dataset should represent the main Dataset.  




%Software's Dataset grouping based on their similarities can be accomplished by using various clustering algorithms such as DBSCAN \cite{ester1996density}, K-means, C-means etc. These clustering algorithms need to plot the Dataset in the two-dimensional space to apply the clustering technique. As, the code metrics have eight properties, so it is totally impossible to plot them into two-dimensional space. To plot these Dataset in two dimensional space, the dimensions of the Dataset are needed to reduced. In this thesis, the dimensions of Dataset are reduced by applying the factor analysis technique using coefficient value.

Software engineering Dataset can be reduced by using various dimension reduction techniques such as principal component analysis (PCA), factor analysis (FA) etc. The PCA uses covariance metrics, its eigenvector and eigenvalue for reducing the dimensions. On the other hand, the FA uses correlation metrics to reduce the dimensions of the Dataset. In the recent year, many researches have used dimension reduction technique to reduce the dimension of the Dataset. Nagappan et al. reduces the dimension of the software engineering Dataset by using PCA to select the best attributes for the prediction model \cite{nagappan2005use,nagappan2006mining,nagappan2007using}. Then it uses those selected attributes for predicting the failure pronness of the software using the code churn and all dependencies' information. Zimmermenn et al. performed an experiments for predicting defects using network analysis of dependency graphs among various pieces of codes \cite{zimmermann2008predicting}. For that purpose, it selects the best set of attributes by reducing the multicollinearity among the Dataset using PCA for the prediction model. Menzies et al. have used PCA to reduce the dimension of the Dataset \cite{menzies2011local,menzies2013local}. It plots the Dataset considering the most variability component as x-axis and the next component as y-axis to apply the WHERE clustering algorithm to find out the similar objects from the Dataset. Although, it uses only two components for plotting the Dataset into two dimensions, it does not clarify whether only two dimensions can describe all the variances.


%There exists some prominent dimension reduction algorithms such as program component analysis, factor analysis to reduce the dimensions of a Dataset. The program component analysis technique uses covariance metrics to produce the components from the Dataset. The total numbers of component returned by program component analysis is less or equal to the dimension of the Dataset. Then the component that can describe the most of the variabilities are used instead of using the whole Dataset. The factor analysis technique uses the correlation matrix to produces latent variables by minimizing the dimensions of a Dataset.      


In this thesis, the dimension of the software engineering Dataset is reduced by using coefficient values  to produce the latent variables. The coefficient values show the significance of independent variables to the dependent variable. If the dimension reduction technique reduces the dimension based on the coefficient values, the produced latent variables' values will be based on their significance to the dependent variables. So, the new dimension reduced Dataset contains closer values for similar objects. If the new dimension reduced Dataset is plotted in the two dimensional space, the similar objects are closer to each others. As a result, different clustering technique can accurately identify clusters from the software engineering Dataset with similar properties.

The experiment has been performed on some open source software such as jEdit, Ant, Xalan etc. from the Promise Repository \cite{promise12}. The proposed technique reduces the dimensions of the Dataset for the different clustering algorithms. After reducing the dimension, the clustering algorithms such as DBSCAN \cite{ester1996density}, Where clustering \cite{menzies2011local,menzies2013local} have been applied to the dimension reduced Dataset to group these into multiple clusters. Then the linear regression model has been applied to each cluster to find out predicted defects. To show the importance and effectiveness of the dimension reduction approach for clustering algorithms, these two clustering approaches also applied to the dimension reduced Dataset by PCA. Finally, results are compared to show how dimension reduction affects clustering and the clustering affects the defect prediction model.

 %using coefficient values on 
 % implemented such as BorderFlow and no clustering approach that considers the whole Dataset. The Where clustering approach \cite{menzies2011local,menzies2013local} is also applied to the dimension reduced Dataset by PCA to show which dimension reduction technique is better for software engineering Dataset.

Results show that the dimension reduction technique using the coefficient values outperforms others. 



\section{Related work}


Dimension Reduction (DR) is a process of reducing a set of variables into minimum number of variables that can explain the data perfectly. The goal of this approach is to find a set of correlated variables to form a new smaller set of latent variables with minimum loss of information. There are lots of ways to perform DR in a Dataset such as Principal Component Analysis (PCA), Feature Selection (FA) etc.

The PCA uses an orthogonal transformation to convert a set of correlated observations into a set linearly uncorrelated variables called principal components \cite{abdi2010principal,wold1987principal,jolliffe2002principal}. It reduces the dimension of a Dataset into minimum number of dimensions that can describes all the varibality of the data. To reduce the dimension, it first subtract the mean of each dimension from the Dataset and generates covariance matrices of the Dataset using all variables. Then it calculates the eigenvector and eigenvalue of the covariance matrices. Finally it chooses components and feature vector from the eigenvalue and eigenvector. Normally, the total number of principal component is less or equal to the number of original dimension of the Dataset.The first principal component is the linear combination of n-variables that has maximum variance, so it gives as much variation in the data as possible. Just like first component, the second principal component is also the linear combination of n-variables maximize the remaining variation as possible, with the constraint that the correlation between the first and second component is $0$. Like first and second component, the i-th principal component maximize the remaining variation of the data. For the software engineering Data like Promise Repository \cite{promise12}, the first and second component can only describe the 30-50\% varibality of the Dataset. So, it will not wise decision to rely on the only first two dimensions because it can only describe 30-50\% of the total variabilities.


%As the goal of PCA is to give minimum dimensional data that can represent the multidimensional data. 

%The first principal component will have the greatest variance, the second will show the second most variance not described by the first, and so forth
The Factor Analysis (FA) is another way of reducing the dimension of a Dataset, like PCA \cite{malinowski2002factor,kim1978introduction}. It is based on the correlation matrix of the variables whether PCA uses covariance matrix. It produces latent variables by joining a set of observed variables. To produce latent variables, it uses the available data, common factors and mean of the data. Then it produces the factor model just like regression's equation, to produce factor loadings. Finally it transforms the factor into latent variables to minimize the dimensions.

 %To reduce the dimension from a Dataset, it first generates correlation matrix and then determines the number of factors needed to represent the whole Dataset.



%It is a statistical method used to describe variability among the correlated variables. 


Nagappan et al. performed an experiment on Windows $2003$ to identify the relationship among software dependencies, churn measures and post-release failures \cite{nagappan2005use,nagappan2007using}. This experiment differs significantly from the previous work as it integrates the architectural dependencies to investigate the propagation of churn across the system. The Code churn is a measure of the amount of code change taking place within a software unit over time. It reduces the multicollinearity among the metrics by using the PCA technique. Then, it analyzes the software dependence ratios and churn measures as early indicators of failure proneness of a software by using logistic regression. Results can predict the post release failures and failure-proneness of the binaries and also show which binaries needs more testing and the code inspections etc.



Nagappan et al. proposed a failure prediction model for investigating the relationship between failure-prone software entities and their complexity measures \cite{nagappan2006mining}. It uses linear regression analysis as the predictor models for identifying failure prone components. It performed an emperical study on five Microsoft software systems. It shows that there is no single set of complexity metrics that could act as a best defect predictor and multicollinearity exists among the complexity metrics. To overcome the multicollinearity problem, it uses principal component analysis to select minimum numbers of metrics for the prediction model. It selects only those metrics properties for which the cumulative variance is 96\%. After selecting the best set of metrics properties, it uses these properties to identify the relation among complexity metrics and failure-proneness. Results shows that complexity metrics can successfully predict post release defects. 



Zimmermen et al. proposed a defect prediction model using network analysis of dependency graphs among various pieces of codes \cite{zimmermann2008predicting}. It uses multiple linear regression analysis as a prediction model for predicting defects. It uses PCA to reduce the multicollinearity among the Dataset and to select the best set of attributes for the defect prediction model. It selects only those principal components, for those the cumulative sample variance greater than 95\%. It performed the experiments on Windows Server $2003$ and results show that complexity metrics and Network measures can predict 30\% and 60\% of these critical binaries respectively.  


Menzies et al. 


For the software engineering data, the existing PCA poorly reduces the dimension and the factor analysis technique which slightly depends on PCA cannot identify factor properly because of the poor fit of PCA. In this experiment, the dimension of a Dataset is reduced by using the significant of independent variables to the dependent variables. It produces latent variables by joining the coefficient values with positive significance and coefficient values with negative significant. 


 %The overall goal of this analysis is to use the software dependence ratios and churn measures as early indicators of failure proneness modeled using logistic regression techniques.
 %In order to identify the relationship between the software dependence ratios, churn measures and post-release failures , It uses Spearman rank correlation between the measures and the post-release failures.

\section{Why Dimension reduction is important in Software Engineering Data}

As the described problem, it has been already mentioned that clustering on software engineering Dataset based on their similarity may improve the defect prediction accuracy. 

lets consider an examination, consisting of five subjects, the first two covering mathematics, the next two on literature, and a comprehensive fifth exam. The overall grade is calculated from the five subject. It seems reasonable that the five grades for a given student will be related. Some students are good at both subjects, some are good at only one, etc. if It is possible to  The goal of this analysis is to determine if there is quantitative evidence that the students' grades on the five different exams are largely determined by only two types of ability.

  
\section{Clustering approaches}
 
\subsection{DBSCAN}
DBSCAN clustering algorithm is a density based clustering approach which finds clusters by checking density reachability of different points. A point q is called density reachable to another point, p, if and only if there is a sequence of n points such as $p_{1}$, $p_{2}$… $p_{n}$ with $p_{1}=p$ and $p_{n}=q$ where each is directly density-reachable from Pi. The clusters, generated from DBSCAN, satisfy the following two property:
\begin{enumerate}
	\item {All points within the cluster are mutually density-connected.}
	\item {If a point is density-reachable from any point of the cluster, it is part of the cluster as well.}
\end{enumerate}
	
DBSCAN algorithm requires two parameters to classify the dataset into different clusters. DBSCAN requires two parameters: $\epsilon$ (eps) and the minimum number of points required to form a dense region (minPts). It starts with an arbitrary starting point that has not been visited. This point's $\epsilon$-neighborhood is retrieved, and if it contains sufficiently many points, a cluster is started. Otherwise, the point is labeled as noise. 

%Note that this point might later be found in a sufficiently sized $\epsilon$-environment of a different point and hence be made part of a cluster.

\subsection{WHERE Cluster}

The WHERE clustering algorithm finds the software artifacts with similar properties. It uses FASTMAP heuristic to find out 

\section{Proposed Methodology}
%Every machine learning algorithm uses existing knowledge to train the prediction model. The proposed defect prediction model uses statistical inference to predict defects, so its success depends on the knowledge it gathers from existing Dataset.
%The proposed approach uses statistical inference such as regression analysis for the prediction purpose because Catal shows that statistical model outperforms the expert estimations and experts cannot work on vary large datasets \cite{catal2009systematic}. 

%Hence, we have some available data, so we first analyze the coefficient values and intercept for the regression equation.

The simple regression analysis simply uses a set of independent variables and one or more dependent variable.  The success of the regression analysis depends on the best fit of the train data. In a research, Menzies et al. show that the prediction model that appears to be useful in global context is often obsolete in local context for defect prediction. So, to avoid conclusion instability, we have proposed an approach to divide the software code metrics into multiple groups. Then the prediction model is applied on each group to predict defects for the new Datasets.  

The software Dataset used in the proposed technique contains eight independent variables that makes impossible to apply different clustering algorithms on the Dataset. So, to apply different clustering algorithms on the existing Dataset, the dimension of the Dataset are needed to reduced. As described above the well known dimension reduction approach such as PCA and FA cannot reduce the dimension properly. Here, a new dimension reduction algorithm based on the FA using coefficient value has been proposed along with a defect prediction model. The whole model can be further divided into the following below steps : 

\subsection{Dimension Reduction Approach} 
%As the considered Dataset contains eight independent variables, so the Dataset cannot be plotted in two dimensional space. This problem is solved here by reducing the dimension of the Dataset based on their significance to the dependent variable. 
 
%To apply different clustering algorithm on the data, the dimensions of the data need to be reduced. In this thesis, the dimension of the data is reduced by using a new approach which is similar to principle component analysis (PCA). To reduce the dimension of the data, it uses the coefficients value of each attribute of the data.

FA reduces the dimension of a Dataset by producing latent variables. Hence the FA using correlation matrix cannot work properly in software engineering Dataset. Here FA is modified by using the coefficient values to produce latent variables. The coefficient values show The significance of one independent variable to the dependent variable. It can easily be calculated by using regression analysis. 

To calculate the coefficient values, the regression analysis using the equation \ref{eq:coeffModel} has been applied to the available Dataset. 
\begin{equation}
\label{eq:coeffModel}
 Y=b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{3}+...+b_{n}x_{n}+c
\end{equation}
here, Y is dependent variable, $b_{1}$...$b_{n}$ be the coefficient values of independent variables,$x_{1}$...$x{_n}$ respectively and c is intercept point of y-axis. 

%The significance of one independent variable to the dependent variable can be easily computed by using regression analysis. To find the significance level, this approach uses the following equation \ref{eq:coeffModel} as the regression model. The equation \ref{eq:coeffModel} is applied to the existing Dataset to find out the coefficients values of each attributes in the Dataset. The equation, that is used for finding coefficients is given below:   The coefficients values mean the impact of the particular independent variable to dependent variable.

The coefficients value can be positive or negative based on the impact of the independent variables to the dependent variable. if the coefficients values positive, it means that the dependent variable's value increase with the increase of independent variable's value. if the coefficients values negative, it means that the dependent variable's value decrease with the increase of independent variable's value. 
 
To reduce the dimension of the Dataset, the coefficient values are then multiplied to the corresponding variable value. This produces the new values that might have some positive and negative values. Then the positive and negative values are summed to produce two latent variables, named as PosValue and NegValue respectively for each row in dataset. As a result, the final Dataset contains only two values for each entry. The PosValue and NegValue are calculated by using the equation \cite{eq:PosValue} and \cite{eq:NegValue} respectively. 
\begin{equation}
\label{eq:PosValue}
 PosValue=\sum{b_{i}x_{i}}
\end{equation} 
where $b_{i}$ is positive.
\begin{equation}
\label{eq:NegValue}
NegValue=\sum{b_{i}x_{i}}
\end{equation}
where $b_{i}$ is negative.

Now, the whole Dataset only have two values based on the positive and negative impact of independent variables to the dependent variable. It is now easily plottable to two dimensional space and any clustering algorithm can be applied to the new Dataset to find out clusters according to the requirements. The whole procedure to reduce the dimension of the Dataset is given in Algorithm \ref{dimention_reduction}.


\begin{algorithm}
\caption{Dimention Reduction Algorithm}
\label{dimention_reduction}
\begin{algorithmic}[1]
\REQUIRE  $ Dataset$, $coefficient_value$, $independent_variable_value=0$, $PosValue=0$,$NegValue=0$ 
		\STATE{ Select the equation \ref{eq:coeffModel}}
		\STATE{ Run the selected equation in the available Dataset}
		\STATE{ Compute the coefficient value of each independent variable}
		
		\FOR{each Dataset}
				
				\FOR{each independent variable in each Dataset}
							\STATE{Find the Coefficient value of the selected variable}
							\IF{ $coefficient_value > 0$}
									\STATE{$independent_variable_value = independent_variable_value \times  coefficient_value$}
									\STATE{$PosValue = PosValue \times  independent_variable_value$}
							\ELSE
							\STATE{$independent_variable_value = independent_variable_value \times  coefficient_value$}
									\STATE{$NegValue = NegValue \times  independent_variable_value$}
									
							\ENDIF
				\ENDFOR
				
				\STATE {$NegValue = NegValue \times (-1)$}
				\RETURN{$PosValue$ , $NegValue$}
		\ENDFOR
		

\end{algorithmic}
\end{algorithm}

When the new Dataset is plot, considering the PosValue as x-axis and NegValue as y-axis. It distributes the OO class in such a way that similar classes are close to each other and dissimilar classes are far from each others. The defective classes are situated far from origin (0,0) and the non-defective classes are close to origin.  

\subsection{Cluster Formation}

The dimension reduction process reduces the attributes of a Dataset and makes it only two dimensional which can easily be illustrated in two dimensional plane. Hence the dimension reduction process reduces the dimension based on their impact value such as coefficient value, so the position of any class depends on its impact factor to dependent variable. It is expected that objects with same possibility of having defects get similar PosValue and NegValue value.
 
In the new value, the equal objects locates closer. So if the total software can be grouped based on the distance measure, then the equal objects will belong to same cluster. For this experiments, to group the whole software into multiple groups based on distance, the density based clustering algorithm (DBSCAN) is used. It is also needed to mention that DBSCAN is not the only solution to this approach. Any clustering algorithm can be used to find clusters in dimension reduced Dataset.
 
The DBSCAN clustering algorithm groups the whole software based on the distance measurement. To make the algorithm workable, a distance needs to be measured manually. For this experiments, the distance, which is known as eps value for DBSCAN, is calculated using the intercept point generated from prediction model.

The eps value calculation for DBSCAN is most critical and important because the success of this cluttering algorithm depends on it. In this experiment, the eps value is calculated by summing all intercept value from the prediction model, described in equation \ref{eq:epsGenerationModel}, considering only one independent variable at a time. 

\begin{equation}
\label{eq:epsGenerationModel}
 Y=bx+c
\end{equation}
Hence, this experiment only uses eight code metrics, described in \ref{Code_Metrics_background_study}, so, the eps value is calculated by summing the eight intercept values, generated from considering one independent variable at a time.  
The whole procedure of calculating the eps value for DBSCAN is given in Algorithm \ref{eps_calculation_algorithm}. 

\begin{equation}
\label{eq:totalepsCalculation}
 eps=\sum{b_{i}}
\end{equation}

\begin{algorithm}
	\caption{eps value calculation for DBSCAN}
	\label{eps_calculation_algorithm}
		\begin{algorithmic}[1]
			\REQUIRE  $ Dataset$, $coefficient_value$, $interceptPoint=0$,$eps=0$, $intercept, c$
				
				\FOR{each independent variables in Dataset}
						\STATE {Run equation \ref{eq:coeffModel} considering one variable at a time}
						\STATE {Calculate $c$ for each variable}
						\STATE { eps=eps+c}
				\ENDFOR
				\RETURN {eps}
		\end{algorithmic}
	\end{algorithm}
	
	
	
\subsection{Prediction Process}
%The prediction model's success depends on the data used for the training of the prediction model. To perform prediction more accurately, the data used for prediction has already been analyzed by using the Algorithm \ref{dimention_reduction}. It's now time to predict the defects of the software using the selected prediction process.

%The proposed prediction process further can be divided into two steps.
%
%\begin{itemize}
	%\item Predicting cluster
	%\item predicting defects
%\end{itemize}
%
%\subsubsection{	Predicting cluster}

The proposed model tries to find out the better way to train the prediction model. The proposed dimesion reduction approach divides the whole Dataset into mulitple clusters to minimize the data variability. So, before predicting defect of a OO class or file, it is important to identify the cluster where the selected file belongs to. To predict the potential cluster where a OO class might exist, it first build a cluster prediction model using the used clustering technique. Then it uses the statistical prediction model to identify the potential cluster where the selected OO file might belong to. When the identification of training data for a OO file is completed, the prediction model use these data to learn to further analysis.
%
%When new objects are available to predict defects, its first classify this objects to the existing clusters that have been calculated. Our cluster prediction approach assigns the new items to different clusters using the following algorithms.  
%2.	Learning from this cluster
%The previous step gives us a set of new item and their corresponding clustering information. In this steps, we iteratively select an item and learnt the prediction model from the corresponding dataset. Call the steps 3. 
%\subsubsection{Predict defects}

To predict defects for a OO class or a whole software, the prediction model first learns from the selected data for the specific class or a whole software. After the completion of learning treatment, the prediction model predicts the defects for the selected OO classes.

The whole prediction process is illustrated in Algorithm \ref{prediction_process}.

\begin{algorithm}
	\caption{Prediction process}
	\label{prediction_process}
		\begin{algorithmic}[1]
			\REQUIRE  $ Dataset$, $coefficient_value$, $interceptPoint=0$,$eps=0$, $intercept, c$
			
			\FOR{ each OO class to be predicted}
					\STATE{ Find the cluster where it belongs to}
					\STATE{Train the prediction model using the selected cluster's data}
					\STATE {Predict defect for the OO file}
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}

\section{Experimental Setup and Results Anlaysis}

In this section, the experimental environment setup and the results analysis of the proposed SDP model using dimension reduction for clustering algorithms will be analyzed. Finally, the effectiveness of the proposed SDP will be measured by taking the mean, variance and standard deviation of absolute residuals of the prediction model. 

\subsection{Experimental Setup}

To perform clustering algorithms on the Dataset, a tool based on the aforementioned dimension reduction technique was implemented to make the two-dimensional Dataset . A lots of open source software such as JEdit, POI, Xalan, Camel, Ant, Tomcat and Synapse were collected from promise repository \cite{promise12} to use in this experiment. After reducing the dimension of the above Dataset, the prominent clustering algorithms such as DBSCAN and  Where clustering approach were applied on the dimension reduced Dataset. Other clustering approach such as BorderFlow was also applied to the available Dataset. The BorderFlow clustering algorithm was implemented as mentioned in \cite{scanniello2013class}. The where clustering approach was also applied to the Dataset using Tim menzies approach that uses PCA for reducing the dimension \cite{menzies2011local,menzies2013local}.
 
The DBSCAN \cite{ester1996density} and Where clustering technique \cite{menzies2011local,menzies2013local} were implemented by using R script. The BorderFlow clustering algorithm was implemented by using Java. The SDP model using linear regression analysis was built by the R statistical environment. These clustering algorithms divided the selected software's Dataset into multiple clusters using their approaches. Finally, the SDP model was then applied to each cluster to predict defects. 

%To perform the experiment, three clustering algorithms such as BorderFlow, DBSCAN and Where were applied on the available Dataset. These three clustering algorithms divided the selected software's Dataset into multiple clusters using their approaches. The BorderFlow clustering algorithm was implemented as mentioned in \cite{scanniello2013class}. The DBSCAN and Where clustering approach have been applied to the Dataset after reducing the dimension of the Dataset according to proposed technique. The where cluster was also applied to the Dataset using Tim menzies approach that uses PCA for reducing the dimension.


\subsection{Results Anlaysis}

The results of different SDP model using different clustering algorithms are compared by using the mean, median and standard deviation of the absolute residuals (AR).  

The descriptive statistics of the absolute residuals are summarized in Table \ref{dimReducedTable} to compare the different defect prediction approaches. It includes n projects statistical description of Absolute residuals (AR). Here, the performance of different clustering technique are analyzed based on the AR values.
%
%The experiments have been performed using R environment \cite{survival-package} and the software Dataset have been collected from the PROMISE Repositiry \cite{promise12}.
%To perform this experiments on the defect Dataset both within porject and cross projects, the software 
%\section{Results Analysis}

\begin{landscape}
\begin{table}[h]
\label{dimReducedTable}
\captionof{table}{The mean, median and standard deviation of AR} \label{results} 
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{DBSCAN}                             & \multicolumn{3}{c|}{Where}                                & \multicolumn{3}{c|}{Tim Menzies}                          & \multicolumn{3}{c|}{BorderFlow}                           & \multicolumn{3}{c|}{Entire System}                       \\ \cline{2-16} 
                         & MAR               & MdAR              & StDev           & MAR               & MdAR              & StDev             & MAR               & MdAR              & StDev             & MAR               & MdAR              & StDev             & MAR               & MdAR             & StDev             \\ \hline
Xalan-2.4 & 0.28392811 & 0.12198353 & 0.457161045 & 0.30610816 & 0.168271448 & 0.44228084 & 0.338213884 & 0.153404025 & 0.477041564 & 0.18216919 & 0.11778425 & 0.35865286 & 0.31240664 & 0.14950500 & 0.50215394 \\ \hline
Xalan-2.5  &  0.621290206 &	0.5329312 &	0.518163806 & 0.839792835 &	0.561608325	& 0.881358511 & 0.801374581 &
	0.511094539	& 0.816273652 & 0.964380065 &	0.520583723	&1.543990905 & 0.664628077 &	0.523784587	&0.56850671   \\ \hline
Xalan-2.6  &   0.721258769	&0.571328058	&0.545263761&
1.144100232	&0.940572791	&0.930836974&
1.521553864	&0.892520813	&1.859690378&
0.770118563	&0.429367917	&1.062586007&
0.685554999	&0.510605331	&0.645265273\\ \hline
Xalan-2.7  & 0.664452516&	0.572281351&	0.4070094&
0.388827782&    0.26339485&	0.37005634&
0.625071619&	0.625169449&	0.45225035&
1.369998553&	1.30029219&     0.391908662&
0.433593607&	0.259546618&	0.498686479 \\ \hline
		Ant-1.7 &0.459996674&	0.242778721&	0.544256052&
0.649938729&	0.480072346&	0.602460821&
0.713764409&	0.29944726&	1.0726034&
0.563805013&	0.205148353&	0.789651157&
0.492987016&	0.186967476&	0.861398133		\\ \hline
Ant-1.6&0.48514463&	0.295516961&	0.586733347&
0.997017809&	1.025944722&	0.699260297&
0.995684125&	0.631867097&	0.981090322&
		&&&
0.590749031	&0.253418183&	0.820902069\\ \hline

Ant-1.5 &0.132165751&	0.102811787&	0.122078028
&0.302755629&	0.056316226&	0.673084118
&0.400237104&	0.146037508&	0.473479935
&0.117396886&	0.084381425&	0.197389864
&0.199592294&	0.106753045&	0.242312833\\ \hline

Ant-1.4 &0.270270749&	0.196350826&	0.222423142&
2.850568371&	0.171613643&	7.680442405&
0.641258283&	0.450989282&	0.576476231&
0.213644458&	0.213330504&	0.137297361&
0.421370627&	0.325435599&	0.410650741   \\ \hline

Ant-1.3  &0.2543912&	0.151580992&	0.317828876&
0.399617194&	0.177575989&	0.50175148&
1.060736999&	0.263067249&	2.825350021&
0.176936539&	0.078760314&	0.340531208&
0.581794897&	0.328314056&	0.799701618    \\ \hline

\end{tabular}
\end{table}
\end{landscape}



\chapter{Conclusion and Future Research Direction}


\begin{singlespace}
\bibliography{references}
\bibliographystyle{IEEEtran}
\end{singlespace}

\end{document}
              